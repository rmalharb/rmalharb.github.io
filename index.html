<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✨</text></svg>">
  <title>Rahaf Alharbi</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Rahaf Alharbi</name>
              </p>
              <p style="text-align: justify">
                I am a Ph.D. candidate in <a href="https://www.si.umich.edu/">the School of Information</a> at the <a href="https://umich.edu/">University of Michigan</a>, where I am advised by <a href="https://robinbrewer.com/">Dr. Robin Brewer</a> and <a href="https://yardi.people.si.umich.edu/">Dr. Sarita Schoenebeck</a>. Broadly, my research lies at the intersection of accessibility and human-computer interaction (HCI). I draw from disability studies and disability justice activism to understand, critique, build, and design real-world technologies. In collaboration with blind communities, a key thread of my research aims to rethink artificial intelligence (AI) tools for visual information access by surfacing and combating privacy and transparency harms. Additionally, I have led and collaborated on projects related to accessible workplaces and multilingual captioning with D/deaf and neurodivergent communities. My scholarly work has been published in top-tier HCI venues, including ACM CHI, ASSETS, and CSCW. 
              </p>

            <!--
              <p style="text-align: justify">

                I work towards a future where disabled people have agency and control over their data. Specifically, my research outlines a disability-centric responsible AI practice by reimaging privacy and transparency techniques in visual assistance technologies. 

              My work draws from disability studies and disability justice to examine the possibilities and limitations of emerging AI technologies for managing privacy and enhancing transparency. Through in-depth qualitative research, I explored <a href="data/privacyBLV.pdf">blind people's perspectives on AI-enabled techniques to detect and redact private content</a>. I found that while blind people recognize potential benefits, they desire greater control over what constitutes as private and are concerned about errors that may be challenging to detect. To design for non-visual transparency, I conducted an interview study to understand <a href="data/ASSETS2024.pdf">how blind people detect and resolve AI errors </a>in their everyday lives. My ongoing work seeks to co-design transparency techniques in emerging AI-enabled privacy tools. I do this research in pursuit of designing AI futures with disabled communities. 
              </p>
              -->

              <p style="text-align: justify">
                I interned at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> with the <a href="https://www.microsoft.com/en-us/research/group/ability/">Ability team</a> and at <a href="https://www.meta.com">Meta</a> with the <a href="https://ai.meta.com/responsible-ai/">Responsible AI team</a>. Prior to graduate school, I obtained my Bachelor of Science degree in Mechanical Engineering (minor in Ethnic Studies) at the <a href="https://www.ucsd.edu/">University of California, San Diego</a>.
              </p>




              <p align="center">
                <a href="mailto:rmalharb@umich.edu">Email</a> &nbsp;/&nbsp;
                <a href="data/Rahaf_cv.pdf">CV (last updated Oct 2024)</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?hl=en&user=mefIyhYAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>
              </p>
            </td>
            <td width="33%">
              <a href="images/profile_photo" aria-label="Rahaf smiling. She is wearing a black blazer, black scarf with brown flowers, and standing in downtown Ann Arbor." class="hoverZoomLink">
                <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" alt="Rahaf smiling. She is wearing a black blazer, black scarf with brown flowers, and standing in downtown Ann Arbor." src="images/profile_photo.jpg">
              </a>
            </td>
          </tr>
        </table>



<div style="overflow : scroll; height:150px;">
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
            <td width="100%" valign="middle">
                <heading>Updates</heading>
                <ul>
                  <li><strong>Oct. 2024:</strong> I am presenting my <a href="data/ASSETS2024.pdf">paper</a> and a <a href="data/Accessible_poster_compress.pdf">doctoral consortium poster</a> at ASSETS 2024 in person! </li>

                  <li><strong>Oct. 2024:</strong> I passed my dissertation proposal! I am now ABD!</li>
                  <li><strong>Oct. 2024:</strong> I was awarded the Gary M. Olson Outstanding PhD Student Award by UMSI!</li>
                    <li><strong>May 2024:</strong> Selected to attend <a href="https://hcic.org/">HCIC</a> as a University of Michigan representative. Super excited to chat with old and new friends!</li>
                    <li><strong>June 2024:</strong> My paper was conditionally accepted to ASSETS 2024! I am really excited to share this work soon on AI accessible verification and visual access.</li>
                    <li><strong>May 2024:</strong> Selected to attend <a href="https://hcic.org/">HCIC</a> as a University of Michigan representative. Super excited to chat with old and new friends!</li>
                    <li><strong>Mar. 2024:</strong> I won the <a href="https://rackham.umich.edu/discover-rackham/announcing-the-2024-2025-rackham-predoctoral-fellowship-awards/">Rackham Predoctoral Fellowship</a> which aims to support dissertations that are "unusually creative, ambitious, and impactful."</li>
                    <li><strong>Aug. 2023:</strong> Our <a href="https://sites.google.com/umich.edu/privacya11y/home?pli=1">privacy and accessibility</a> workshop was accepted to ASSETS 2023!</li>
                    <li><strong>May 2023:</strong> Started my internship at Meta in the Responsible AI team! I am excited to be back in California!</li>
                    <li><strong>Mar. 2023:</strong> Excited to present our paper “Accessibility Barriers, Conflicts, and Repairs: Understanding the Experience of Professionals with Disabilities in Hybrid Meetings” at CHI 2023 in Hamburg, Germany.</li>
                    <li><strong>Dec. 2022:</strong> I passed my prelim defense! I’m now a Ph.D. candidate!</li>
                    <li><strong>May 2022:</strong> I started my internship at Microsoft Research in the Ability team!</li>
                    <li><strong>Apr. 2022:</strong> My first first-author paper was accepted to CSCW 2022! Excited to present my study on the benefits and harms that Blind people perceive of future privacy technology (obfuscation).</li>
                    <li><strong>Nov. 2022:</strong> I passed my pre-candidacy defense!</li>
                </ul>
            </td>
        </tr>
    </table>
</div>

   
<section>
  <h1>Selected Journal and Conference Publications</h1> 
  
  <article class="publication">  
    <div style="display: flex; align-items: center;">
      <div style="flex: 1; padding: 20px">
        <img src="images/assets2024.jpg" alt="decision tree explaining a blind person's verification process of using sensory skills, comparing with visual assistance technology, cross-referencing with another visual assistance technologies, and if requires accuracy or includes security risks, verifying with sighted people" width="240" style="max-width: 100%;border-style: none;">
      </div>
      <div style="flex: 2;">
        <papertitle>Misfitting With AI: How Blind People Verify and Contest AI Errors</papertitle>
        <p><u><strong>Rahaf Alharbi</strong></u>, Pa Lor, Jaylin Herskovitz, Sarita Schoenebeck, Robin Brewer</p>
        <p><em>ASSETS 2024</em></p>
        <a href="data/ASSETS2024.pdf">PDF</a> 
        <p>We interviewed 26 blind people to understand how they make sense of errors in AI-enabled visual assistance technologies. We described common errors such as processing issues and cross-cultural bias. Blind people developed tactics to readdress and identify AI errors such as everyday experimentation in low-risk settings and strategically involving sighted people. We drew from disability studies framework of misfitting and fitting to expand our findings, and inform responsible AI scholarship.</p>
      </div>
    </div>
  </article>

  <article class="publication">  
    <div style="display: flex; align-items: center;">
      <div style="flex: 1; padding: 20px">
        <img src="images/P21_exp_2.jpg" alt="Illustration of Deaf ASL user that has a laptop and a mobile device setup. On the laptop, there is a video conferencing interface that shows the video grid of in-person attendees and three other remote attendees. Also, there is a mobile device, standing upright, with a video image of an ASL interpreter." width="240" style="max-width: 100%;border-style: none;">
      </div>
      <div style="flex: 2;">
        <papertitle>Accessibility Barriers, Conflicts, and Repairs: Understanding the Experience of Professionals with Disabilities in Hybrid Meetings</papertitle>
        <p><u><strong>Rahaf Alharbi</strong></u>, John Tang, Karl Henderson</p>
        <p><em>CHI 2023</em></p>
        <a href="data/accessible_chi23-846.pdf">PDF</a> /
        <a href="https://dl.acm.org/doi/10.1145/3544548.3581541">ACM DL</a> /
        <a href="https://www.youtube.com/watch?v=FtMdTMvDSbE">Talk</a>
        <p>We interviewed 21 professionals with disabilities to unpack the accessibility dimensions of hybrid meetings. Our analysis demonstrates how invisible and visible access labor may support or undermine accessibility in hybrid meetings. We offer practical suggestions and design directions to make hybrid meetings accessible.</p>
      </div>
    </div>
  </article>
  
  <article class="publication">
    <div style="display: flex; align-items: center;">
      <div style="flex: 1; padding: 20px;">
        <img src="images/CHI2023_DIY_AT.jpg" alt="Illustration of participant trying to use Seeing AI to read mail, but they frustrated because Seeing AI keeps repeating the same information as they slightly shift their camera" width="240" style="max-width: 100%; border-style: none;">
      </div>
      <div style="flex: 2;">
        <papertitle>Hacking, Switching, Combining: Understanding and Supporting DIY Assistive Technology Design by Blind People</papertitle>
        <p>Jaylin Herskovitz, Andi Xu, <u><strong>Rahaf Alharbi</strong></u>, Anhong Guo</p>
        <p><em>CHI 2023</em></p>
        <a href="data/CHI2023_DIY_AT.pdf">PDF</a> /
        <a href="https://dl.acm.org/doi/10.1145/3543216.3545660">ACM DL</a> /
        <a href="https://www.youtube.com/watch?v=6JcgFpf1edE">Talk</a> /
        <a href="https://github.com/HumanAILab/diy-a11y">Dataset</a>
        <p>Current assistive technologies (AT) often fail to support the unique needs of Blind people, so they often 'hack' and create Do-it-Yourself (DIY) AT. To further understand and support DIY AT, we conducted two-stage interviews and diary study with 12 Blind participants and we present design considerations for future DIY technology systems to support existing customization and creation process of Blind people.</p>
      </div>
    </div>
  </article>

  <article class="publication">
    <div style="display: flex; align-items: center;">
      <div style="flex: 1; padding: 20px;">
        <img src="images/firstmonday.jpg" alt="First Monday logo" width="240" style="max-width: 100%; border-style: none;">
      </div>
      <div style="flex: 2;">
        <papertitle>Definition Drives Design: Disability Models and Mechanisms of Bias in AI Technologies</papertitle>
        <p>Denis Newman-Griffis, Jessica Sage Rauchberg, <u><strong>Rahaf Alharbi</strong></u>, Louise Hickman, Harry Hochheiser</p>
        <p><em>First Monday</em></p>
        <a href="data/newm.pdf">PDF</a> /
        <a href="https://firstmonday.org/ojs/index.php/fm/article/view/12903">First Monday DL</a> 
        <p>We reveal how AI bias stems from various design choices, including problem definition, data selection, technology use, and operational elements. We show that differing disability definitions drive distinct design decisions and AI biases. Our analysis offers a framework for scrutinizing AI in decision-making and promotes disability-led design for equitable AI.</p>
      </div>
    </div>
  </article>

  <article class="publication">
    <div style="display: flex; align-items: center;">
      <div style="flex: 1; padding: 20px;">
        <img src="images/CSCWgif.gif" alt="GIF of a medicine bottle with the patient name being obfuscated by blurring" width="240" style="max-width: 100%; border-style: none;">
      </div>
      <div style="flex: 2;">
        <papertitle>Understanding Emerging Obfuscation Technologies in Visual Description Services for Blind and Low Vision People</papertitle>
        <p><u><strong>Rahaf Alharbi</strong></u>, Robin N. Brewer, Sarita Schoenebeck</p>
        <p><em>CSCW 2022</em></p>
        <a href="data/privacyBLV.pdf">PDF</a> /
        <a href="https://dl.acm.org/doi/10.1145/3555570">ACM DL</a> /
        <a href="https://www.youtube.com/watch?v=Z-aRxKsN6J8">Talk</a> 
        <p>Machine learning approaches such as obfuscation are often thought of as the state-of-art solution to addressing visual privacy concerns. We interviewed 20 Blind and low vision people to understand their perspectives on obfuscation. We found that while obfuscation may be beneficial, it imposes significant trust and accessibility issues. Participants worried that cultural or gendered privacy needs might be overlooked in obfuscation systems. We applied the framework of interdependence to rethink current obfuscation approaches, and provided more inclusive design directions.</p>
      </div>
    </div>
  </article>

</section>

  

  <!-- Footer -->
  <footer style="text-align: center;">
    <p>&copy; Code by <a href="https://jonbarron.info">Jon Barron</a>. Thank you!</p>
  </footer>
</body>
</html>


               
